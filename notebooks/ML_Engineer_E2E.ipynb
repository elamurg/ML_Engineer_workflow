{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55992c76",
   "metadata": {},
   "source": [
    "\n",
    "# End-to-End Machine Learning Engineer Workflow (Google/Industry Style)\n",
    "\n",
    "**Date:** 2025-10-14 19:04 UTC  \n",
    "\n",
    "This notebook is an *advanced*, realistic walkthrough of how a Machine Learning Engineer might work end-to-end: from problem framing through deployment and monitoring. It’s designed to be a reusable template that you can adapt to your project.\n",
    "\n",
    "> This is **tool-agnostic** but uses Google Cloud primitives when relevant (e.g., Vertex AI, BigQuery, GCS). Everything here also maps well to open-source stacks (Kubernetes, MLflow, Feast, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b0e620",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Typical Tooling & Resources\n",
    "\n",
    "**IDEs / Dev Environments**\n",
    "- Google Colab / Colab Pro\n",
    "- VS Code (with Remote SSH / Dev Containers), PyCharm\n",
    "- Cloud Shell Editor (in-browser)\n",
    "- JupyterLab\n",
    "\n",
    "**Languages & Libraries**\n",
    "- Python 3.x, Conda or `pip` venvs\n",
    "- Data: `pandas`, `numpy`, `pyarrow`, `polars`\n",
    "- Modeling: `scikit-learn`, `xgboost`, `lightgbm`, `tensorflow`/`keras`, `jax`/`flax`\n",
    "- Time series: `statsmodels`, `prophet` (or `neuralprophet`), `tsfresh`\n",
    "- Experiment tracking: `mlflow`, `Weights & Biases` (W&B), Vertex AI Experiments\n",
    "- Pipelines/Orchestration: **TFX**, **Vertex AI Pipelines**, **Kubeflow Pipelines**, **Apache Airflow**\n",
    "- Feature store: **Feast** (OSS) or Vertex AI Feature Store\n",
    "- Model serving: **Vertex AI Endpoints**, **Cloud Run**, **GKE** (Kubernetes), **TF Serving**\n",
    "- Testing/Quality: `pytest`, `great_expectations`, `pydantic`, `evidently`\n",
    "- Monitoring: **Vertex AI Model Monitoring**, **Prometheus/Grafana**, **Evidently**, custom logs\n",
    "\n",
    "**Cloud (Google Cloud Platform)**\n",
    "- **BigQuery** (data warehouse / SQL analytics)\n",
    "- **Cloud Storage (GCS)** (data & artifact storage)\n",
    "- **Dataflow**/**Apache Beam** (stream/batch data pipelines)\n",
    "- **Pub/Sub** (event streaming)\n",
    "- **Vertex AI** (training, hyperparameter tuning, pipelines, feature store, endpoints, monitoring)\n",
    "- **Cloud Build** + **Artifact Registry** (CI/CD, container images)\n",
    "- **GKE**/**Cloud Run** (containerized services)\n",
    "\n",
    "**CI/CD & Reproducibility**\n",
    "- GitHub/GitLab/Cloud Source Repositories\n",
    "- Cloud Build / GitHub Actions\n",
    "- Docker, Make, `pyproject.toml` + `tox`\n",
    "- Infra-as-code: Terraform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d41daf",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Problem Framing & Success Criteria\n",
    "\n",
    "**Business objective:** e.g., *Forecast daily demand for Category X to reduce stockouts by 15% and overstock by 10% within the next quarter.*\n",
    "\n",
    "**ML task type:** Regression (time series); KPI = RMSE (primary), MAE, MAPE/SMAPE, R².\n",
    "\n",
    "**Constraints & SLAs**\n",
    "- Latency: < 200ms P95 for online predictions\n",
    "- Throughput: 200 RPS\n",
    "- Model refresh cadence: daily retrain\n",
    "- Fairness/ethics: ensure no harmful bias in allocation decisions\n",
    "- Privacy/compliance: PII handling, GDPR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71ca2f",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Repository Layout (Recommended)\n",
    "\n",
    "```\n",
    "repo/\n",
    "├─ notebooks/\n",
    "├─ src/\n",
    "│  ├─ data/\n",
    "│  │  ├─ ingest.py\n",
    "│  │  ├─ validate.py\n",
    "│  ├─ features/\n",
    "│  │  ├─ build.py\n",
    "│  ├─ models/\n",
    "│  │  ├─ train.py\n",
    "│  │  ├─ evaluate.py\n",
    "│  │  ├─ infer.py\n",
    "│  ├─ serving/\n",
    "│  │  ├─ service.py\n",
    "│  ├─ utils/\n",
    "│  │  ├─ io.py\n",
    "│  │  ├─ metrics.py\n",
    "│  └─ config.py\n",
    "├─ tests/\n",
    "│  ├─ test_data_validations.py\n",
    "│  ├─ test_feature_pipeline.py\n",
    "│  ├─ test_train_eval.py\n",
    "├─ Dockerfile\n",
    "├─ pyproject.toml / setup.py\n",
    "├─ requirements.txt\n",
    "├─ Makefile\n",
    "└─ README.md\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1915ea1",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Data Ingestion & Validation\n",
    "\n",
    "Below we simulate loading data from CSV. In practice you might:\n",
    "- Query **BigQuery** using `pandas-gbq` or the BigQuery Python client.\n",
    "- Load from **GCS** (`gs://bucket/path`) via `gcsfs` or `google-cloud-storage`.\n",
    "- Stream from **Pub/Sub** or process via **Dataflow/Beam**.\n",
    "\n",
    "Always add **schema/data quality checks** (e.g., `great_expectations`, `pydantic`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Simulated) Data Ingestion\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic example: time series with covariates\n",
    "rng = pd.date_range(\"2022-01-01\", periods=600, freq=\"D\")\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    \"date\": rng,\n",
    "    \"feature_search_interest\": np.clip(np.sin(np.linspace(0, 30, len(rng))) * 50 + 50 + np.random.randn(len(rng))*5, 0, None),\n",
    "    \"feature_promo\": np.random.binomial(1, 0.1, size=len(rng)),\n",
    "    \"feature_price\": np.clip(100 + np.random.randn(len(rng))*3, 0, None)\n",
    "})\n",
    "# target depends on features with noise\n",
    "df[\"target_demand\"] = (\n",
    "    0.6*df[\"feature_search_interest\"] -\n",
    "    0.3*df[\"feature_price\"] +\n",
    "    20*df[\"feature_promo\"] +\n",
    "    np.random.randn(len(rng))*8 +\n",
    "    30\n",
    ").clip(0)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d59298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic schema/quality checks\n",
    "assert df.isna().sum().sum() == 0, \"Found missing values\"\n",
    "assert (df[\"date\"].diff().dropna() > pd.Timedelta(0)).all(), \"Dates must be increasing\"\n",
    "print(\"Basic validations passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb3dff",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Time-Aware Split (Train/Val/Test)\n",
    "\n",
    "For time series:\n",
    "- Train: oldest 70%\n",
    "- Val: next 15%\n",
    "- Test: most recent 15%\n",
    "\n",
    "This avoids leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time-aware split\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "n = len(df)\n",
    "train_end = int(n*0.70)\n",
    "val_end = int(n*0.85)\n",
    "\n",
    "train = df.iloc[:train_end]\n",
    "val   = df.iloc[train_end:val_end]\n",
    "test  = df.iloc[val_end:]\n",
    "\n",
    "features = [\"feature_search_interest\", \"feature_promo\", \"feature_price\"]\n",
    "target = \"target_demand\"\n",
    "\n",
    "X_train, y_train = train[features], train[target]\n",
    "X_val, y_val     = val[features], val[target]\n",
    "X_test, y_test   = test[features], test[target]\n",
    "\n",
    "len(train), len(val), len(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b0015",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Feature Engineering Pipeline\n",
    "\n",
    "Use `ColumnTransformer` and `Pipeline` for reproducibility.  \n",
    "For sequence models (LSTM/JAX/TF), build windows; for tabular, keep as-is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94179c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric = [\"feature_search_interest\", \"feature_price\"]\n",
    "categorical = [\"feature_promo\"]\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]), numeric),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical)\n",
    "])\n",
    "\n",
    "preprocess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0dcca",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Baseline Model (XGBoost) & Metrics\n",
    "\n",
    "Start with a strong baseline (often gradient boosting). This is your anchor before deep models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552786ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    booster = XGBRegressor(\n",
    "        n_estimators=400, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.9, colsample_bytree=0.9, random_state=42\n",
    "    )\n",
    "except Exception as e:\n",
    "    # Fallback to RandomForest if xgboost not available\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    booster = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", booster)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred_val = pipe.predict(X_val)\n",
    "\n",
    "rmse = mean_squared_error(y_val, pred_val, squared=False)\n",
    "mae  = mean_absolute_error(y_val, pred_val)\n",
    "r2   = r2_score(y_val, pred_val)\n",
    "\n",
    "print(f\"Validation RMSE: {rmse:.3f}\")\n",
    "print(f\"Validation MAE : {mae:.3f}\")\n",
    "print(f\"Validation R²  : {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856b478",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Hyperparameter Tuning (RandomizedSearchCV)\n",
    "\n",
    "In production you might use **Vertex AI Vizier**, **KerasTuner**, **Optuna**, or **Ray Tune**.  \n",
    "Here we show a lightweight scikit-learn randomized search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64238d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_distributions = {\n",
    "    \"model__n_estimators\": randint(200, 800),\n",
    "    # for XGBRegressor; for RF these will be ignored or adapted\n",
    "    \"model__max_depth\": randint(3, 10),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe, param_distributions=param_distributions,\n",
    "    n_iter=8, scoring=\"neg_root_mean_squared_error\",\n",
    "    refit=True, random_state=42, n_jobs=-1, cv=3\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "pred_val_best = best_model.predict(X_val)\n",
    "rmse_best = mean_squared_error(y_val, pred_val_best, squared=False)\n",
    "mae_best  = mean_absolute_error(y_val, pred_val_best)\n",
    "r2_best   = r2_score(y_val, pred_val_best)\n",
    "\n",
    "print(\"Best Params:\", search.best_params_)\n",
    "print(f\"Validation RMSE (best): {rmse_best:.3f}\")\n",
    "print(f\"Validation MAE  (best): {mae_best:.3f}\")\n",
    "print(f\"Validation R²   (best): {r2_best:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d05c61",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Deep Model (TensorFlow) — Optional\n",
    "\n",
    "For sequences, you might build an LSTM/Temporal CNN/Transformer. Below is a **template**; adjust windowing to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff788d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Template only — may require TensorFlow installed in your environment.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "\n",
    "    # Build a simple MLP on tabular features as an example (replace with proper sequence windows for LSTM)\n",
    "    tf_model = models.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1)  # linear output for regression\n",
    "    ])\n",
    "    tf_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "    # Use preprocessed features to feed TF model (fit the preprocesser first)\n",
    "    X_train_p = preprocess.fit_transform(X_train)\n",
    "    X_val_p = preprocess.transform(X_val)\n",
    "\n",
    "    history = tf_model.fit(\n",
    "        X_train_p, y_train,\n",
    "        validation_data=(X_val_p, y_val),\n",
    "        epochs=30, batch_size=32, verbose=0\n",
    "    )\n",
    "    print(\"TF model trained (template).\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available in this environment; skipping deep model. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63547c8",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Final Evaluation on Unseen Test Data\n",
    "\n",
    "Select the best model by validation performance, then evaluate once on the **holdout test** set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_test = best_model.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, pred_test, squared=False)\n",
    "mae_test  = mean_absolute_error(y_test, pred_test)\n",
    "r2_test   = r2_score(y_test, pred_test)\n",
    "\n",
    "print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "print(f\"Test MAE : {mae_test:.3f}\")\n",
    "print(f\"Test R²  : {r2_test:.3f}\")\n",
    "\n",
    "# Residual plot\n",
    "resid = y_test - pred_test\n",
    "plt.figure()\n",
    "plt.scatter(pred_test, resid, alpha=0.6)\n",
    "plt.axhline(0, linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted (Test)\")\n",
    "plt.show()\n",
    "\n",
    "# Prediction vs Actual over time\n",
    "plt.figure()\n",
    "plt.plot(test[\"date\"], y_test.values, label=\"Actual\")\n",
    "plt.plot(test[\"date\"], pred_test, label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted (Test)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5242ab",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Experiment Tracking\n",
    "\n",
    "Options:\n",
    "- **MLflow**: model registry, experiment UI\n",
    "- **Vertex AI Experiments**: native on GCP\n",
    "- **Weights & Biases (W&B)**\n",
    "\n",
    "> In production at Google scale, you would log: datasets (URIs), code commit, params, metrics, artifacts, and environment hashes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec2f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example MLflow usage (requires mlflow installed & a tracking server)\n",
    "# import mlflow\n",
    "# mlflow.set_experiment(\"demand_forecasting\")\n",
    "# with mlflow.start_run():\n",
    "#     mlflow.log_params(search.best_params_)\n",
    "#     mlflow.log_metric(\"rmse_val\", rmse_best)\n",
    "#     mlflow.log_metric(\"mae_val\", mae_best)\n",
    "#     mlflow.log_metric(\"r2_val\", r2_best)\n",
    "#     mlflow.sklearn.log_model(best_model, \"model\")\n",
    "#     mlflow.log_artifact(\"path/to/data_schema.json\")\n",
    "# print(\"Logged to MLflow.\")\n",
    "print(\"MLflow example commented out — enable if available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49053064",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Persist Artifacts\n",
    "\n",
    "Save the model, preprocessors, and metadata. Store in **GCS** in production (`gs://...`) and register in a model registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, joblib, json, pathlib\n",
    "\n",
    "art_dir = pathlib.Path(\"artifacts\")\n",
    "art_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model, art_dir / \"model.joblib\")\n",
    "joblib.dump(preprocess, art_dir / \"preprocess.joblib\")\n",
    "meta = {\n",
    "    \"created_utc\": \"2025-10-14T19:04:47.162400Z\",\n",
    "    \"features\": [\"feature_search_interest\", \"feature_promo\", \"feature_price\"]\n",
    "}\n",
    "with open(art_dir / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "sorted(os.listdir(art_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6326d350",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Serving API (FastAPI) — Local or Cloud Run/GKE\n",
    "\n",
    "You can containerize this and deploy to **Cloud Run** or **GKE**. For Vertex AI, use **Endpoints**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example FastAPI app (save as src/serving/service.py in a real repo)\n",
    "fastapi_example = r'''\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI()\n",
    "model = joblib.load(\"artifacts/model.joblib\")\n",
    "preprocess = joblib.load(\"artifacts/preprocess.joblib\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(payload: dict):\n",
    "    X = pd.DataFrame([payload])\n",
    "    Xp = preprocess.transform(X)\n",
    "    y = model.predict(X)\n",
    "    return {\"prediction\": float(y[0])}\n",
    "'''\n",
    "print(fastapi_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd0cf05",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Dockerfile (for Cloud Run/GKE)\n",
    "\n",
    "Build and push with **Cloud Build** or GitHub Actions. Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2826d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dockerfile = r'''\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "COPY artifacts/ artifacts/\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY src/serving/service.py service.py\n",
    "EXPOSE 8080\n",
    "CMD [\"python\", \"-m\", \"uvicorn\", \"service:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "'''\n",
    "print(dockerfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5e941",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Vertex AI Deployment (High-Level)\n",
    "\n",
    "- Upload model artifact to GCS.\n",
    "- Create a **Model** resource in Vertex AI, point to artifact.\n",
    "- Deploy to an **Endpoint** with autoscaling.\n",
    "- Optionally enable **Model Monitoring** (drift/anomaly).\n",
    "\n",
    "Example (Python, unexecuted here):\n",
    "```python\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=\"YOUR_PROJECT\", location=\"us-central1\")\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"demand-forecast-model\",\n",
    "    artifact_uri=\"gs://your-bucket/artifacts/\",\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-4:latest\",\n",
    ")\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-2\",\n",
    "    traffic_split={\"0\": 100},\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73202413",
   "metadata": {},
   "source": [
    "\n",
    "## 15) Testing Strategy\n",
    "\n",
    "- **Unit tests** for data validators, feature pipelines, custom metrics (`pytest`).\n",
    "- **Integration tests**: end-to-end pipeline on a small sample.\n",
    "- **Shadow testing**: send real traffic to the new model but don’t serve results.\n",
    "- **Canary release / A/B tests**: split % of users to new model; compare KPIs.\n",
    "- **Load tests**: ensure latency & throughput SLAs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e732ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tiny example \"unit test\" (in-notebook) for metrics\n",
    "def rmse(y_true, y_pred):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "# Sanity check\n",
    "_pred = np.array([1.0, 2.0, 3.0])\n",
    "_true = np.array([1.0, 2.0, 4.0])\n",
    "assert abs(rmse(_true, _pred) - (1/3)**0.5) < 1e-6\n",
    "print(\"Metric unit test passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac649ca",
   "metadata": {},
   "source": [
    "\n",
    "## 16) Post-Deployment Monitoring\n",
    "\n",
    "- **Data drift**: compare live feature distributions to training (Evidently, Vertex Monitoring).\n",
    "- **Performance drift**: monitor RMSE/MAE on delayed ground truth.\n",
    "- **Logging/Tracing**: request logs, latency, error rates (Cloud Logging, Prometheus).\n",
    "- **Alerting**: SLO breaches trigger rollbacks or retraining.\n",
    "\n",
    "**Retraining loop**: schedule pipelines (Vertex AI Pipelines/Airflow) to refresh data → features → train → evaluate → deploy if better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec5081",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### What to customize next\n",
    "- Replace synthetic data with your BigQuery query.\n",
    "- Swap baseline model for LSTM/Transformer with proper windowing.\n",
    "- Wire up MLflow or Vertex AI Experiments for full tracking.\n",
    "- Containerize and deploy to **Cloud Run** or **Vertex AI Endpoints**.\n",
    "- Add CI/CD (Cloud Build) + automated tests.\n",
    "\n",
    "> Save this notebook as your project template and iterate! 🚀\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
